[
["index.html", "Introduction to Computational and Data Sciences Frontmatter", " Introduction to Computational and Data Sciences James K. Glasbrenner Ajay Kulkarni 2018-11-26 Frontmatter Introduction to Computational and Data Sciences is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["r-session-information.html", "R session information", " R session information The R session information when compiling this book is shown below: ## R version 3.5.0 (2017-01-27) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 14.04.5 LTS ## ## Locale: ## LC_CTYPE=en_US.UTF-8 ## LC_NUMERIC=C ## LC_TIME=en_US.UTF-8 ## LC_COLLATE=en_US.UTF-8 ## LC_MONETARY=en_US.UTF-8 ## LC_MESSAGES=en_US.UTF-8 ## LC_PAPER=en_US.UTF-8 ## LC_NAME=C ## LC_ADDRESS=C ## LC_TELEPHONE=C ## LC_MEASUREMENT=en_US.UTF-8 ## LC_IDENTIFICATION=C ## ## Package version: ## bookdown_0.7.24 dplyr_0.7.99.9000 ## ggplot2_3.1.0.9000 infer_0.4.0 ## kableExtra_0.9.0 knitr_1.20 ## readr_1.2.1.9000 rmarkdown_1.10.16 ## tinytex_0.9 ## ## Pandoc version: 2.2.1 "],
["toolbox-introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction The following chapters will help you become familiar with the software and online platforms that will be used during the course. "],
["github.html", "Chapter 2 GitHub ", " Chapter 2 GitHub "],
["getting-started.html", "2.1 Getting started", " 2.1 Getting started 2.1.1 Account sign-up To create an account on GitHub, begin by launching your web browser and navigate to https://github.com/. In the signup form, enter your your Mason @gmu.edu email address, a username, and a password, and then click on the “sign up for GitHub” button After creating the account on GitHub, you will see a new page containing details about plans for repositories. Keep the default options and click on the “Continue” button. The next page asks you to provide information about your programming experience and other details. This is optional. To skip this step, click on the “skip this step” link. Otherwise, once you are done entering the information, click the “Submit” button. Your Github account is now created and you will be greeted with the welcome page. You will also receive an email about account verification. Please click on the link in the email to verify your account. 2.1.2 Joining the class organization The next step after creating your account is to join the course organization. You should have received an email with a link to join the class organization on GitHub at the start of this semester. You should receive an email from the course instructor to join the class organization on GitHub at the start of the semester. In that email, there will be a link you need to click in order to join the organization. If you cannot find the email, please contact your course instructor. After clicking the link, you may be greeted with this page. This means your browser doesn’t remember your sign-in, or you haven’t signed up yet, which you should now do if that’s the case. If you have an account, then click the sign-in link at the bottom and enter your username and password. Once you are logged in, you will be asked to approve the necessary permissions so that you can join the class organization, which you should do. "],
["navigating-the-github-site.html", "2.2 Navigating the GitHub site", " 2.2 Navigating the GitHub site For your convenience, the course website contains links taking you directly to course content stored on GitHub. However, you may prefer to access your existing content by navigating the GitHub site itself, so let’s take a short tour of the GitHub interface. 2.2.1 Main dashboard When you will login into GitHub for the first time you will see the main dashboard for the website. At the top of the dashboard there are several buttons, boxes, and icons that you can click in order to navigate the site. Starting from the left, GitHub icon: This is similar to the “Home” or “Main” buttons found on other websites. Clicking this icon brings you back to your currently active dashboard, which is what we’re already viewing. Search bar: Used to look for content on the GitHub site, such as repositories and user accounts, just click it and start typing. Pull requests, Issues, Marketplace, and Explore: Of these links, the only one you might use for the course is “Pull requests”, however we’ll save the discussion about Pull requests for another time. Bell icon: This displays your account notifications, such as when someone tags your username. Plus icon: Used for creating content, such as creating a new repository. The last icon on the right is your profile picture, and clicking it opens up another menu containing some additional links. From this menu you can reach your profile, account settings, the help section, and a couple other of other pages. Let’s take a quick look to see what the profile and settings pages look like. 2.2.2 Profile page Clicking Your profile will take you to your account’s profile page. Since you most likely have a new account, there won’t be much here right now. If you click the Repositories tab at the top of your profile, it will display a list of repositories associated with your account, however keep in mind that it will not have any content you work on for the class. This is important to keep in mind when you are looking for your files! 2.2.3 Settings Clicking Settings takes you to the account settings page where you can inspect and update your account settings. Use the menu on the left to choose the setting you would like to change. For now you may want to upload a profile picture or add a short bio under the Profile setting. You can also change how the site sends you email updates and notification alerts under the Emails and Notifications settings. 2.2.4 Class organization page Returning to the main dashboard, after you’ve joined the class organization and started using the site some more, your dashboard will begin to look more like this. As we learned while looking at the profile page, your classwork will not be visible there. Instead, you will need to navigate to the class organization in order to find your files for the class, so let’s do that now. Click the gray icon with your username on the left side of the page, as outlined in the above screenshot, which will open up a dropdown menu. This menu lists all the GitHub organizations your account belongs to, and you can identify our class organization as it will have cds-101 somewhere in its name. Clicking the class organization in the menu will bring you to the class organization dashboard. The dashboard is similar to the dashboard we saw earlier, but it will only contain content associated with the class. On the left you will see a list of some of your class repositories, which you can filter by typing in the Find a repository… search box. If you don’t see any repositories there, don’t worry, we will get around to creating them soon enough! To see a full list of your class repositories, similar to the repositories tab on your profile page, click the View organization button on the upper right of the page. Here you should be able to find all of the repositories that you’ve used during the class. Keep in mind If you click the GitHub icon while you are accessing content in the class organization, it will bring you back to the class organization dashboard, not the main dashboard we saw earlier. To get back to the main dashboard, click the gray icon with the class organization name and then click your username in the dropdown menu. "],
["repositories.html", "2.3 Repositories", " 2.3 Repositories When you access a repository on GitHub, the page will look similar to the screenshot below, The page contains a lot of information about the repository along with multiple tabs and buttons, which can be a bit overwhelming at first. As it turns out, we will not need to use many of these tabs or buttons during the class, so instead let’s focus on the most important ones, which have been highlighted in the red and blue rectangles. Red Code tab: Clicking this brings you back to the main repository page, which is what is displayed in the above screenshot Commit tab: takes you to the commit history for the repository, which are the series of “snapshots” that you create using the git tool in RStudio Blue Dropdown branch menu: use this to inspect a branch that is different from the default master branch Clone or download button: provides a link to use when obtaining a copy of a repository. For the class, you will do this by creating a new project in RStudio using the version control option. Pull requests tab: generally used for code reviews and quality control when a user wants to contribute code to a repository. For the class, pull requests will be used to submit your work so that the instructor is able to leave line-by-line comments about your code. Below the tabs and button is a list of files stored in the repository, Each repository will have different files. Clicking a file’s name will bring you to another page that shows a preview of the file contents. The descriptions in the middle of the file list show the most recent commit message for each file and the timespan on the right shows how recently the file was last updated. The above file list also shows you what you’ll see in a folder after you first obtain a copy of the repository. In that way, each repository can be thought of as a folder containing files, The advantage of this approach is that each repository you create is isolated and separate, which helps to reduce certain kinds of coding errors. Below the repository file list is a rendered version of the README.md file, The README.md file describes the contents of the repository and can be used as a form of documentation. It is a good idea to look at the README.md file of any repository you visit on GitHub to see if it gives examples or quick instructions on how to set up and use the files. "],
["additional-topics.html", "2.4 Additional topics", " 2.4 Additional topics 2.4.1 How to create a repository We saw in the last tutorial that after creating an account on GitHub, there are three ways to create a repository. The most common method of creating a repository is to click on “+” sign and then click on “New repository” option. After clicking on “New repository” you will go to a new screen where you need to give a repository name. You will also find two options to either keep repository as public or private. In the end, there will be another option to initialize the repository with Readme.md file. GitHub doesn’t allow to create a repository without a name. Thus, it is mandatory to give a name to the repository. After providing all the details and selecting appropriate options click on “Create repository” button. In this way, the repository will be successfully created. If you have ticked on initializing the repository with Readme.md file, then you will find the file in the repository. You can easily edit Readme.md file by clicking on the file. After clicking Readme.md file if you want to add any content in that file then click on the pencil symbol. Click on “Commit” button after making changes in the file. 2.4.2 How to import a repository To import a repository from other account click on “+” sign and then click on “Import repository” option. After that, you need to provide a link of other repository which you need to clone or import and the repository name to store on your GitHub account. After giving all the details click on “Begin import” button. The import process will start, and you will get a notification after completion of the import process. In the end, you will be able to see the imported repository in your GitHub account. "],
["describing-numerical-data.html", "Chapter 3 Describing numerical data", " Chapter 3 Describing numerical data Many of the figures that we will be creating and analyzing during the course will be representations of univariate (meaning one variable) and bivariate (meaning two variables) data. You will frequently be asked to write a description about a visualization, and you should aim to be precise and consistent in your terms. Use the short summaries below as a guide and a reminder when writing about the features contained in a univariate or bivariate plot. Describing univariate data When describing the visual properties of univariate data, remember to discuss the following traits: shape: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail) unimodal, bimodal, multimodal, uniform center: mean (mean), median (median), mode (not always useful) spread: range (range), standard deviation (sd), inter-quartile range (IQR) unusual observations For additional guidance, follow this link for a summary of what the above terms mean: http://stattrek.com/statistics/charts/data-patterns.aspx Describing bivariate data When describing the visual properties of univariate data, you will frequently be looking at a scatterplot. When describing the shapes of scatterplots we highlight: Direction: What direction is the data trending? Positive direction or negative direction? Form: This is analogous to shape for univariate data. Is the dataset linear? Is is curved? Does it not have a form? Strength: How clustered are the data points around the underlying form? Stated another way, what are the strength of the correlations? Typical descriptors are strong, moderate, or weak. "],
["representing-distributions.html", "Chapter 4 Representing distributions", " Chapter 4 Representing distributions This chapter introduces us to two more ways we can represent univariate (single variable) data distributions as we start to learn how to compare two or more distributions and draw conclusions. So far we’ve focused on representing univariate distributions using frequency histograms (by default, geom_histogram() sorts data into different bins and tells you how many end up in each one). Frequency histograms are useful for examining the particulars of a single variable, but have limited utility when directly comparing distributions that contain different numbers of observations. We resolve this issue by introducing the normalized version of the frequency histogram, the probability mass function (PMF). As we’ll see, the PMF still has its limitations, which will motivate us to consider other, more robust representations of data distributions, such as the very useful cumulative distribution function (CDF). # Packages library(dplyr) library(ggplot2) library(readr) # Dataset county_complete &lt;- read_rds(path = &quot;data/county_complete.rds&quot;) "],
["probability-mass-functions.html", "4.1 Probability mass functions", " 4.1 Probability mass functions 4.1.1 Example dataset We use an example dataset of the average time it takes for people to commute to work across 3143 counties in the United States (collected between 2006-2010) to help illustrate the meaning and uses of the probability mass function. The frequency histogram for these times can be plotted using the following code snippet: county_complete %&gt;% ggplot(mapping = aes(x = mean_work_travel)) + geom_histogram(binwidth = 1) 4.1.2 PMFs The probability mass function (PMF) represents a distribution by sorting the data into bins (much like the frequency histogram) and then associates a probability with each bin in the distribution. A probability is a frequency expressed as a fraction of the sample size n. Therefore we can directly convert a frequency histogram to a PMF by dividing the count in each bin by the sample size n. This process is called normalization. As an example, consider the following short sample, 1 2 2 3 5 If we choose a binwidth of 1, then we get a frequency histogram that looks like this: There are 5 observations in this sample. So, we can convert to a PMF by dividing the count within each bin by 5, getting a histogram that looks like this: The relative shape stays the same, but compare the values along the vertical axis between the two figures. You’ll find that they are no longer integers and are instead probabilities. The normalization procedure (dividing by 5) guarantees that adding together the probabilities of all bins will equal 1. For this example, we find that the probability of drawing the number 1 is 0.2, drawing 2 is 0.4, drawing 3 is 0.2, drawing 4 is 0, and drawing 5 is 0.2. That is the biggest difference between a frequency histogram and a PMF, the frequency histogram maps from values to integer counters, while the PMF maps from values to fractional probabilities. 4.1.3 Plotting PMFs The syntax for plotting a PMF using ggplot2 is nearly identical to what you would use to create a frequency histogram. The one modification is that you need to include y = ..density.. inside aes(). As a simple example, let’s take the full distribution of the average work travel times from earlier and plot it as a PMF: county_complete %&gt;% ggplot(mapping = aes(x = mean_work_travel, y = ..density..)) + geom_histogram(binwidth = 1) Let’s do a comparison to show how one might use a PMF for analysis. For example, we could ask if two midwestern states such as Nebraska and Iowa have the same distribution of work travel times, or if there is a meaningful difference between the two. First, let’s filter the dataset to only include these two states: nebraska_iowa &lt;- county_complete %&gt;% filter(state == &quot;Iowa&quot; | state == &quot;Nebraska&quot;) Now let’s plot the frequency histogram: nebraska_iowa %&gt;% ggplot() + geom_histogram( mapping = aes( x = mean_work_travel, fill = state, color = state ), position = &quot;identity&quot;, alpha = 0.5, binwidth = 1 ) The position = &quot;identity&quot; input overlaps the two distributions (instead of stacking them) and alpha = 0.5 makes the distributions translucent, so that you can see both despite the overlap. On our first glance, it looks like the center of the Nebraska times is lower than the center of the Iowa times, and that both have a long tail on the right-hand side. However, if we do a count summary, nebraska_iowa %&gt;% count(state) state n Alabama 0 Alaska 0 Arizona 0 Arkansas 0 California 0 Colorado 0 Connecticut 0 Delaware 0 District of Columbia 0 Florida 0 Georgia 0 Hawaii 0 Idaho 0 Illinois 0 Indiana 0 Iowa 99 Kansas 0 Kentucky 0 Louisiana 0 Maine 0 Maryland 0 Massachusetts 0 Michigan 0 Minnesota 0 Mississippi 0 Missouri 0 Montana 0 Nebraska 93 Nevada 0 New Hampshire 0 New Jersey 0 New Mexico 0 New York 0 North Carolina 0 North Dakota 0 Ohio 0 Oklahoma 0 Oregon 0 Pennsylvania 0 Rhode Island 0 South Carolina 0 South Dakota 0 Tennessee 0 Texas 0 Utah 0 Vermont 0 Virginia 0 Washington 0 West Virginia 0 Wisconsin 0 Wyoming 0 we find that the two states do not have the exact same number of counties, although they are close in this particular example. Nonetheless, any comparisons should be done using a PMF in order to account for differences in the sample size. We use the following code to create a PMF plot: nebraska_iowa %&gt;% ggplot() + geom_histogram( mapping = aes( x = mean_work_travel, y = ..density.., fill = state, color = state ), position = &quot;identity&quot;, alpha = 0.5, binwidth = 1 ) The trend that the center of the travel times in Nebraska is slightly smaller than in Iowa continues to hold even after converting to a PMF. To provide an example where a PMF is clearly necessary, what if we compare New Jersey with Virginia? Virginia has many more counties than New Jersey: county_complete %&gt;% filter(state == &quot;New Jersey&quot; | state == &quot;Virginia&quot;) %&gt;% count(state) state n Alabama 0 Alaska 0 Arizona 0 Arkansas 0 California 0 Colorado 0 Connecticut 0 Delaware 0 District of Columbia 0 Florida 0 Georgia 0 Hawaii 0 Idaho 0 Illinois 0 Indiana 0 Iowa 0 Kansas 0 Kentucky 0 Louisiana 0 Maine 0 Maryland 0 Massachusetts 0 Michigan 0 Minnesota 0 Mississippi 0 Missouri 0 Montana 0 Nebraska 0 Nevada 0 New Hampshire 0 New Jersey 21 New Mexico 0 New York 0 North Carolina 0 North Dakota 0 Ohio 0 Oklahoma 0 Oregon 0 Pennsylvania 0 Rhode Island 0 South Carolina 0 South Dakota 0 Tennessee 0 Texas 0 Utah 0 Vermont 0 Virginia 134 Washington 0 West Virginia 0 Wisconsin 0 Wyoming 0 As a result, comparing their frequency histograms gives you this: The New Jersey distribution is dwarfed by the Virginia distribution and it makes it difficult to make comparisons. However, if we instead compare PMFs, we get this: So, for example, we can now make statements like “a randomly selected resident in New Jersey is twice as likely as a randomly chosen resident in Virginia to have an average work travel time of 30 minutes.” The PMF allows for an “apples-to-apples” comparison of the average travel times. "],
["cumulative-distribution-functions.html", "4.2 Cumulative distribution functions", " 4.2 Cumulative distribution functions 4.2.1 The limits of probability mass functions Probability mass functions (PMFs) work well if the number of unique values is small. But as the number of unique values increases, the probability associated with each value gets smaller and the effect of random noise increases. Let’s recall that, in the previous reading, we plotted and compared PMFs of the average work travel time in Virginia and New Jersey, which resulted in this figure: What happens if we choose binwidth = 0.1 for plotting the mean_work_travel distribution? The values in mean_work_travel are reported to the first decimal place, so binwidth = 0.1 does not “smooth out” the data. This increases the number of distinct values in mean_work_travel from 41 to 304. The comparison between the Virginia and New Jersey PMFs will then look like this: This visualization has a lot of spikes of similar heights, which makes this difficult to interpret and limits its usefulness. Also, it can be hard to see overall patterns; for example, what is the approximate difference in means between these two distributions? This illustrates the tradeoff when using histograms and PMFs for visualizing single variables. If we smooth things out by using larger bin sizes, then we can lose information that may be useful. On the other hand, using small bin sizes creates plots like the one above, which is of limited (if any) utility. An alternative that avoids these problems is the cumulative distribution function (CDF), which we turn to describing next. But before we discuss CDFs, we first have to understand the concept of percentiles. 4.2.2 Percentiles If you have taken a standardized test, you probably got your results in the form of a raw score and a percentile rank. In this context, the percentile rank is the fraction of people who scored lower than you (or the same). So if you are “in the 90th percentile”, you did as well as or better than 90% of the people who took the exam. As an example, say that you and 4 other people took a test and received the following scores: 55 66 77 88 99 If you received the score of 88, then what is your percentile rank? We can calculate it as follows: test_scores &lt;- data_frame(score = combine(55, 66, 77, 88, 99)) number_of_tests &lt;- test_scores %&gt;% count() %&gt;% pull(n) number_of_lower_scores &lt;- test_scores %&gt;% filter(score &lt;= 88) %&gt;% count() %&gt;% pull(n) percentile_rank &lt;- 100.0 * number_of_lower_scores / number_of_tests From this, we find that the percentile rank for a score of 88 is 80. Mathematically, the calculation is \\(100 \\times \\dfrac{4}{5} = 80\\). As you can see, if you are given a value, it is easy to find its percentile rank; going the other way is slightly harder. One way to do this is to sort the scores and find the row number that corresponds to a percentile rank. To find the row number, divide the total number of scores by 100, multiply that number by the desired percentile rank, and then round up to the nearest integer value. The rounding up operation can be handled via the ceiling() function. So, for our example, the value with percentile rank 55 is: percentile_rank_row_number &lt;- ceiling(55 * number_of_tests / 100) test_scores %&gt;% arrange(score) %&gt;% slice(percentile_rank_row_number) score 77 The result of this calculation is called a percentile. So this means that, in the distribution of exam scores, the 55th percentile corresponds to a score of 77. In R, there is a function called quantile() that can do the above calculation automatically, although you need to take care with the inputs. Let’s first show what happens when we aren’t careful. We might think that we can calculate the 55th percentile by running: test_scores %&gt;% pull(score) %&gt;% quantile(probs = combine(0.55)) x 55% 79.2 We get a score of 79.2, which isn’t in our dataset. This happens because quantile() interpolates between the scores by default. Sometimes you will want this behavior, other times you will not. When the dataset is this small, it doesn’t make as much sense to permit interpolation, as it can be based on rather aggressive assumptions about what intermediate scores might look like. To tell quantile() to compute scores in the same manner as we did above, add the input type = 1: test_scores %&gt;% pull(score) %&gt;% quantile(probs = combine(0.55), type = 1) x 55% 77 This, as expected, agrees with the manual calculation. It is worth emphasizing that the difference between “percentile” and “percentile rank” can be confusing, and people do not always use the terms precisely. To summarize, if we want to know the percentage of people obtained scores equal to or lower than ours, then we are computing a percentile rank. If we start with a percentile, then we are computing the score in the distribution that corresponds with it. 4.2.3 CDFs Now that we understand percentiles and percentile ranks, we are ready to tackle the cumulative distribution function (CDF). The CDF is the function that maps from a value to its percentile rank. To find the CDF for any particular value in our distribution, we compute the fraction of values in the distribution less than or equal to our selected value. Computing this is similar to how we calculated the percentile rank, except that the result is a probability in the range 0 – 1 rather than a percentile rank in the range 0 – 100. For our test scores example, we can manually compute the CDF in the following way: test_scores_cdf &lt;- test_scores %&gt;% arrange(score) %&gt;% mutate(cdf = row_number() / n()) score cdf 55 0.2 66 0.4 77 0.6 88 0.8 99 1.0 The visualization of the CDF looks like: As you can see, the CDF of a sample looks like a sequence of steps. Appropriately enough, this is called a step function, and the CDF of any sample is a step function. Also note that we can evaluate the CDF for any value, not just values that appear in the sample. If we select a value that is less than the smallest value in the sample, then the CDF is 0. If we select a value that is greater than the largest value, then the CDF is 1. 4.2.4 Representing CDFs While it’s good to know how to manually compute the CDF, we can construct the CDF of a sample automatically using the geom_step() function if we include stat = &quot;ecdf&quot; as an additional input. Let’s use this to look at the CDF for the average work travel time for the full dataset: county_complete %&gt;% ggplot() + geom_step(mapping = aes(x = mean_work_travel), stat = &quot;ecdf&quot;) + labs(y = &quot;CDF&quot;) With this plot we can easily specify an average work travel time percentile and read the associated time from the plot and vice-versa. 4.2.5 Comparing CDFs CDFs are especially useful for comparing distributions. Let’s revisit the comparison we made between the average work travel times in Nebraska and Iowa. Here is the full code that converts those distributions into CDFs: county_complete %&gt;% filter(state == &quot;Nebraska&quot; | state == &quot;Iowa&quot;) %&gt;% ggplot() + geom_step( mapping = aes(x = mean_work_travel, color = state), stat = &quot;ecdf&quot; ) + labs(y = &quot;CDF&quot;) This visualization makes the shapes of the distributions and the relative differences between them much clearer. We see that Nebraska has shorter average work travel times for most of the distribution, at least until you reach an average time of 25 minutes, after which the Nebraska and Iowa distributions become similar to one another. While it takes some time to get used to CDFs, it is worth the effort to do so as they show more information, more clearly, than PMFs. "],
["credits.html", "4.3 Credits", " 4.3 Credits This chapter, “Representing distributions,” is a derivative of “Chapter 3 Probability mass functions” and “Chapter 4 Cumulative distribution functions” in Allen B. Downey, Think Stats: Exploratory Data Analysis, 2nd ed. (O’Reilly Media, Sebastopol, CA, 2014), used under CC BY-NC-SA 4.0. "],
["statistical-inference-with-infer.html", "Chapter 5 Statistical inference with infer ", " Chapter 5 Statistical inference with infer "],
["case-study-comparing-work-travel-times.html", "5.1 Case study: Comparing work travel times", " 5.1 Case study: Comparing work travel times The following is a worked example using the county complete dataset from Section 4 that demonstrates how to use the infer package to test if the difference in mean values of two distributions is statistically significant. To follow along, load the following libraries and dataset into your R session: # Packages library(dplyr) library(ggplot2) library(readr) library(infer) # Dataset county_complete &lt;- read_rds( url(&quot;http://data.cds101.com/county_complete.rds&quot;) ) 5.1.1 Work travel times in Iowa and Nebraska In Section 4, we looked at the distribution of average work travel times in Iowa and Nebraska measured on the county level. In our visual inspection of the probability mass function, we concluded that the distribution of work travel times is shifted slightly towards shorter commutes in Nebraska versus Iowa. The summary statistics for the two distributions seem to support this conclusion: # Filter dataset to only include counties in Iowa and Nebraska ia_ne_county &lt;- county_complete %&gt;% filter(state == &quot;Iowa&quot; | state == &quot;Nebraska&quot;) # Compute summary statistics of mean_work_travel column for Iowa and Nebraska ia_ne_summary_stats &lt;- ia_ne_county %&gt;% group_by(state) %&gt;% summarize( mean = mean(mean_work_travel), median = median(mean_work_travel), sd = sd(mean_work_travel), iqr = IQR(mean_work_travel) ) state mean median sd iqr Alabama NaN NA NA NA Alaska NaN NA NA NA Arizona NaN NA NA NA Arkansas NaN NA NA NA California NaN NA NA NA Colorado NaN NA NA NA Connecticut NaN NA NA NA Delaware NaN NA NA NA District of Columbia NaN NA NA NA Florida NaN NA NA NA Georgia NaN NA NA NA Hawaii NaN NA NA NA Idaho NaN NA NA NA Illinois NaN NA NA NA Indiana NaN NA NA NA Iowa 19.11 18.5 3.425 4.4 Kansas NaN NA NA NA Kentucky NaN NA NA NA Louisiana NaN NA NA NA Maine NaN NA NA NA Maryland NaN NA NA NA Massachusetts NaN NA NA NA Michigan NaN NA NA NA Minnesota NaN NA NA NA Mississippi NaN NA NA NA Missouri NaN NA NA NA Montana NaN NA NA NA Nebraska 17.97 17.6 3.823 5.0 Nevada NaN NA NA NA New Hampshire NaN NA NA NA New Jersey NaN NA NA NA New Mexico NaN NA NA NA New York NaN NA NA NA North Carolina NaN NA NA NA North Dakota NaN NA NA NA Ohio NaN NA NA NA Oklahoma NaN NA NA NA Oregon NaN NA NA NA Pennsylvania NaN NA NA NA Rhode Island NaN NA NA NA South Carolina NaN NA NA NA South Dakota NaN NA NA NA Tennessee NaN NA NA NA Texas NaN NA NA NA Utah NaN NA NA NA Vermont NaN NA NA NA Virginia NaN NA NA NA Washington NaN NA NA NA West Virginia NaN NA NA NA Wisconsin NaN NA NA NA Wyoming NaN NA NA NA Let’s compute the difference in means directly using ia_ne_summary_stats. First, we need to grab the mean column: ia_ne_means &lt;- ia_ne_summary_stats %&gt;% pull(mean) ia_ne_means is a simple vector that stores two numbers. ia_ne_means ## [1] NaN NaN NaN NaN NaN NaN NaN NaN ## [9] NaN NaN NaN NaN NaN NaN NaN 19.11 ## [17] NaN NaN NaN NaN NaN NaN NaN NaN ## [25] NaN NaN NaN 17.97 NaN NaN NaN NaN ## [33] NaN NaN NaN NaN NaN NaN NaN NaN ## [41] NaN NaN NaN NaN NaN NaN NaN NaN ## [49] NaN NaN NaN The first number of the vector, which we can access with ia_ne_means[1], is the mean work travel times in Iowa. The second number of the vector, which we can access with ia_ne_means[2], is the mean work travel times in Nebraska. Knowing this, we easily compute the difference in means, ia_ne_diff_in_means &lt;- ia_ne_means[1] - ia_ne_means[2] obtaining the following value of ia_ne_diff_in_means: # (mean work travel time in Iowa) - (mean work travel time in Nebraska) ia_ne_diff_in_means = NaN minutes However, only checking the numerical difference between the mean values is not enough if we want to claim there is a statistically significant difference in commute times between Iowa and Nebraska. There is significant overlap between the two distributions, with the standard deviation (sd) of each distribution being 3 – 4 times larger than the difference in means. Before we can conclude that the difference in means is statistically significant, we need to estimate how likely it is that this difference could have arisen from chance alone. 5.1.2 Defining the hypothesis test To determine whether or not there is a meaningful difference in means between the two distributions, we will conduct a two-sided hypothesis test using the following null and alternative hypotheses: Null hypothesis: There is no difference between the mean work travel time in Iowa and the mean work travel time in Nebraska. \\[\\text{H}_{0}:\\mu_{\\text{IA}}-\\mu_{\\text{NE}}=0\\] Alternative hypothesis: There is a difference between the mean work travel time in Iowa and the mean work travel time in Nebraska. \\[\\text{H}_{\\text{A}}:\\mu_{\\text{IA}}-\\mu_{\\text{NE}}\\neq{}0\\] We set our significance level to \\(\\alpha=0.05\\), which we will use as a cutoff for determining whether or not a result is statistically significant. 5.1.3 Building the null distribution To conduct our hypothesis test, we first must generate our null distribution using the infer package: ia_ne_mean_work_travel_null &lt;- ia_ne_county %&gt;% specify(formula = mean_work_travel ~ state) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = combine(&quot;Iowa&quot;, &quot;Nebraska&quot;)) Let’s explain what each of these lines is doing. We start by piping our filtered dataset stored in ia_ne_county into specify: specify(formula = mean_work_travel ~ state) Here we specify the response variable and the explanatory variable in the formula = input keyword. The formula syntax is given as follows: response ~ explanatory Therefore, in our formula mean_work_travel ~ state, mean_work_travel is the response variable and state is the explanatory variable. How do we know to use mean_work_travel as the response variable? It’s because of how we formulated our null and alternative hypotheses, such that we are testing whether a person’s mean work travel time is affected by the state he or she lives in, not whether the state that a person lives in is affected by his or her mean work travel time. Finally, because the response variable is numerical, we don’t use the success = keyword in specify. The next line that we pipe into is: hypothesize(null = &quot;independence&quot;) Here we indicate that we are conducting a hypothesis test to check whether or not the variables provided in specify are independent of one another. A basic rule of thumb is, if you specify both an explanatory and a response variable in the formula of specify, then you will use null = &quot;independence&quot;. Next we pipe into this line: generate(reps = 10000, type = &quot;permute&quot;) Here we indicate that we want to permute the mean_work_travel and state columns 10,000 times to build up our null distribution. In effect, we are randomly shuffling the rows in mean_work_travel into a different order, and then randomly shuffling the rows in state into yet another order, which should remove any connections between the mean_work_travel and state columns (this is what we mean by null distribution). We know to use type = &quot;permute&quot; because we have specified both a response and an explanatory variable and we want to see what happens when we assume the two columns are independent. As for reps = 10000, this controls the overall accuracy of our hypothesis tests. There is no hard and fast rule for value to set, but if you want more accurate results, increase reps. However, keep in mind that the larger reps is, the longer it will take to complete the calculation. We finish up by piping into the last line: calculate(stat = &quot;diff in means&quot;, order = combine(&quot;Iowa&quot;, &quot;Nebraska&quot;)) Here we indicate that we are comparing the difference in means between our two distributions (the mean work travel times in Iowa and Nebraska). The input order = combine(&quot;Iowa&quot;, &quot;Nebraska&quot;) defines the subtraction order, so we will find the difference in means by subtracting the mean time in Iowa from the mean time in Nebraska, which is the same order we used for ia_ne_diff_in_means &lt;- ia_ne_means[1] - ia_ne_means[2]. As for the stat input, we know to use stat = &quot;diff in means&quot; because of how we formulated our null and alternative hypotheses, i.e. we are comparing the difference in mean work travel times between Iowa and Nebraska. The way we know that we can’t use something like stat = &quot;diff in props&quot; is because at least one of our variables in specify is numerical (mean_work_travel). The input stat = &quot;diff in props&quot; only applies if both the explanatory and response variables are categorical. The null distribution that we generated looks as follows: ia_ne_mean_work_travel_null %&gt;% visualize() + labs( x = &quot;difference in means&quot;, title = &quot;Difference in mean work travel times null distribution&quot; ) 5.1.4 Computing the two-sided p-value After we’ve generated our null distribution, we can compute the p-value using get_p_value. To compute a p-value, we need an observed statistic, which is simply the difference in means we computed using the dataset itself. This is what we computed using ia_ne_diff_in_means &lt;- ia_ne_means[1] - ia_ne_means[2]. ia_ne_p_value_right &lt;- ia_ne_mean_work_travel_null %&gt;% get_p_value(obs_stat = ia_ne_diff_in_means, direction = &quot;right&quot;) Since we are conducting a two-sided hypothesis test, we need to also get the p-value for the opposite case of ia_ne_means[2] - ia_ne_means[1], which is just -ia_ne_diff_in_means: ia_ne_p_value_left &lt;- ia_ne_mean_work_travel_null %&gt;% get_p_value(obs_stat = -ia_ne_diff_in_means, direction = &quot;left&quot;) The full p-value for a two-sided hypothesis test is the sum of the “left” and “right” p-values we just obtained: ia_ne_p_value_two_sided &lt;- ia_ne_p_value_left + ia_ne_p_value_right p_value NA Our two-sided p-value is NA, which is NA our significance level of \\(\\alpha=0.05\\). We therefore NA the null hypothesis in favor of the alternative hypothesis. The difference between the mean work travel times in Iowa and Nebraska NA statistically significant. We visualize the meaning of the p-value in the following way: ia_ne_mean_work_travel_null %&gt;% visualize() + shade_p_value(obs_stat = ia_ne_diff_in_means, direction = &quot;right&quot;) + shade_p_value(obs_stat = -ia_ne_diff_in_means, direction = &quot;left&quot;) + labs( x = &quot;difference in means&quot;, title = &quot;Difference in mean work travel times null distribution&quot; ) The portions of the null distribution that are in the shaded red area correspond to results that are more extreme than the observed values of NaN and NaN. The more that a null distribution lies within the red portions of the visualization, the more likely it becomes that we will fail to reject the null distribution and the difference in means that we observed in the dataset arose due to random chance alone. 5.1.5 Computing the 95% confidence interval In addition to the two-sided hypothesis test, it is also good practice to compute the 95% confidence interval for the difference in mean work travel times between Iowa and Nebraska. To compute this, we need to generate the bootstrap distribution for the difference in means: ia_ne_mean_work_travel_bootstrap &lt;- ia_ne_county %&gt;% specify(formula = mean_work_travel ~ state) %&gt;% generate(reps = 10000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = combine(&quot;Iowa&quot;, &quot;Nebraska&quot;)) You’ll notice that the code to generate the bootstrap distribution is pretty similar to the code used to generate the null distribution. All we had to do was remove the hypothesize(null = &quot;independence&quot;) line and change the type = keyword in generate from &quot;permute&quot; to &quot;bootstrap&quot;, and that’s it! You might be wondering, how are the methods for generating the null distribution and bootstrap distribution different? For this example, they differ in two important ways: Unlike when we generated the null distribution, the mean_work_travel and state columns are not randomly shuffled Generating a bootstrap distribution requires sampling with replacement, meaning we grab different rows in ia_ne_county at random (we might even grab the same row more than), which we do not do if we permute the columns The bootstrap distribution we just generated looks as follows: ia_ne_mean_work_travel_bootstrap %&gt;% visualize() + labs( x = &quot;difference in means&quot;, title = &quot;Difference in mean work travel times bootstrap distribution&quot; ) This shows what could happen if we went out and collected another set of samples of the data in this dataset. Due to random variations in the people we survey, sometimes we might end up with a dataset where the difference between the mean work travel times in Iowa and Nebraska is closer to 2 minutes instead of 1 minute, or it might be closer to zero. Most of the time, it will be close to 1 minute. The 95% confidence interval is defined as the fraction of the data in the bootstrap distribution that lies between the 2.5th and 97.5th percentiles: ia_ne_ci95 &lt;- ia_ne_mean_work_travel_bootstrap %&gt;% get_confidence_interval() 2.5% 97.5% 0.1167 2.153 This means that, if we keep collecting new samples of mean work travel times in Iowa and Nebraska, we expect that 95% of our samples will have a difference in mean work travel times that lies between 0.1167 and 2.1534. Also note that the lower bound of the confidence interval does not intersect with a difference of means equal to zero, lending further support to the conclusion we reached using our two-sided hypothesis test. We close this example by visualizing the 95% confidence interval by shading the middle 95% of the data in the bootstrap distribution: ia_ne_mean_work_travel_bootstrap %&gt;% visualize() + shade_confidence_interval(endpoints = ia_ne_ci95) + labs( x = &quot;difference in means&quot;, title = &quot;Difference in mean work travel times bootstrap distribution&quot; ) "],
["references.html", "References", " References "]
]
